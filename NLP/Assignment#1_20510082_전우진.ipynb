{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Woojin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Woojin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Woojin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Woojin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Woojin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Woojin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word_tokenize\n",
      "['Your',\n",
      " 'time',\n",
      " 'is',\n",
      " 'limited',\n",
      " ',',\n",
      " 'so',\n",
      " 'do',\n",
      " \"n't\",\n",
      " 'waste',\n",
      " 'it',\n",
      " 'living',\n",
      " 'someone',\n",
      " 'else',\n",
      " \"'s\",\n",
      " 'life',\n",
      " '.',\n",
      " 'Do',\n",
      " \"n't\",\n",
      " 'be',\n",
      " 'trapped',\n",
      " 'by',\n",
      " 'dogma',\n",
      " '–',\n",
      " 'which',\n",
      " 'is',\n",
      " 'living',\n",
      " 'with',\n",
      " 'the',\n",
      " 'results',\n",
      " 'of',\n",
      " 'other',\n",
      " 'people',\n",
      " \"'s\",\n",
      " 'thinking',\n",
      " '.']\n",
      "\n",
      "\n",
      "Wordpunct_tokenize\n",
      "['Your',\n",
      " 'time',\n",
      " 'is',\n",
      " 'limited',\n",
      " ',',\n",
      " 'so',\n",
      " 'don',\n",
      " \"'\",\n",
      " 't',\n",
      " 'waste',\n",
      " 'it',\n",
      " 'living',\n",
      " 'someone',\n",
      " 'else',\n",
      " \"'\",\n",
      " 's',\n",
      " 'life',\n",
      " '.',\n",
      " 'Don',\n",
      " \"'\",\n",
      " 't',\n",
      " 'be',\n",
      " 'trapped',\n",
      " 'by',\n",
      " 'dogma',\n",
      " '–',\n",
      " 'which',\n",
      " 'is',\n",
      " 'living',\n",
      " 'with',\n",
      " 'the',\n",
      " 'results',\n",
      " 'of',\n",
      " 'other',\n",
      " 'people',\n",
      " \"'\",\n",
      " 's',\n",
      " 'thinking',\n",
      " '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize, wordpunct_tokenize\n",
    "\n",
    "sentences = \"\"\"\n",
    "Your time is limited, so don't waste it living someone else's life. \n",
    "Don't be trapped by dogma – which is living with the results of other people's thinking.\"\"\"\n",
    "\n",
    "word_tokenize_result = word_tokenize(sentences)\n",
    "wordpunct_tokenize_result = wordpunct_tokenize(sentences)\n",
    "\n",
    "print(\"Word_tokenize\")\n",
    "pprint(word_tokenize_result)\n",
    "print(\"\\n\")\n",
    "print(\"Wordpunct_tokenize\")\n",
    "pprint(wordpunct_tokenize_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-speech (POS) tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Your', 'PRP$'),\n",
      " ('time', 'NN'),\n",
      " ('is', 'VBZ'),\n",
      " ('limited', 'VBN'),\n",
      " (',', ','),\n",
      " ('so', 'IN'),\n",
      " ('don', 'JJ'),\n",
      " (\"'\", 'POS'),\n",
      " ('t', 'NN'),\n",
      " ('waste', 'NN'),\n",
      " ('it', 'PRP'),\n",
      " ('living', 'VBG'),\n",
      " ('someone', 'NN'),\n",
      " ('else', 'RB'),\n",
      " (\"'\", 'POS'),\n",
      " ('s', 'JJ'),\n",
      " ('life', 'NN'),\n",
      " ('.', '.'),\n",
      " ('Don', 'NNP'),\n",
      " (\"'\", 'POS'),\n",
      " ('t', 'NN'),\n",
      " ('be', 'VB'),\n",
      " ('trapped', 'VBN'),\n",
      " ('by', 'IN'),\n",
      " ('dogma', 'NN'),\n",
      " ('–', 'NN'),\n",
      " ('which', 'WDT'),\n",
      " ('is', 'VBZ'),\n",
      " ('living', 'VBG'),\n",
      " ('with', 'IN'),\n",
      " ('the', 'DT'),\n",
      " ('results', 'NNS'),\n",
      " ('of', 'IN'),\n",
      " ('other', 'JJ'),\n",
      " ('people', 'NNS'),\n",
      " (\"'\", 'POS'),\n",
      " ('s', 'NN'),\n",
      " ('thinking', 'NN'),\n",
      " ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "pos_result=nltk.pos_tag(nltk.tokenize.wordpunct_tokenize(sentences))\n",
    "pprint(pos_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your time is limit , so do n't wast it live someon els 's life . do n't be trap by dogma – which is live with the result of other peopl 's think .\n",
      "['your', 'time', 'is', 'limit', ',', 'so', 'do', \"n't\", 'wast', 'it', 'live', 'someon', 'els', \"'s\", 'life', '.', 'do', \"n't\", 'be', 'trap', 'by', 'dogma', '–', 'which', 'is', 'live', 'with', 'the', 'result', 'of', 'other', 'peopl', \"'s\", 'think', '.']\n",
      "yo tim is limit , so do n't wast it liv someon els 's lif . do n't be trap by dogm – which is liv with the result of oth peopl 's think .\n",
      "['yo', 'tim', 'is', 'limit', ',', 'so', 'do', \"n't\", 'wast', 'it', 'liv', 'someon', 'els', \"'s\", 'lif', '.', 'do', \"n't\", 'be', 'trap', 'by', 'dogm', '–', 'which', 'is', 'liv', 'with', 'the', 'result', 'of', 'oth', 'peopl', \"'s\", 'think', '.']\n",
      "your time is limit , so do n't wast it live someon els 's life . Do n't be trap by dogma – which is live with the result of other peopl 's think .\n",
      "['your', 'time', 'is', 'limit', ',', 'so', 'do', \"n't\", 'wast', 'it', 'live', 'someon', 'els', \"'s\", 'life', '.', 'Do', \"n't\", 'be', 'trap', 'by', 'dogma', '–', 'which', 'is', 'live', 'with', 'the', 'result', 'of', 'other', 'peopl', \"'s\", 'think', '.']\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "text = list(nltk.word_tokenize(sentences))\n",
    "\n",
    "snowball = SnowballStemmer('english')\n",
    "lancaster = LancasterStemmer()\n",
    "porter = PorterStemmer()\n",
    "\n",
    "for stemmer in (snowball, lancaster, porter):\n",
    "    stemmed_text = [stemmer.stem(t) for t in text]\n",
    "    print(\" \".join(stemmed_text))\n",
    "    print(stemmed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your time is limited , so do n't waste it living someone else 's life . Do n't be trapped by dogma – which is living with the result of other people 's thinking .\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# Note: use part of speech tag, we'll see this in machine learning! \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = [lemmatizer.lemmatize(t) for t in text]\n",
    "print(\" \".join(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['time', 'limit', 'waste', 'live', 'someone', 'else', 'life', 'trap', 'dogma', '–', 'live', 'result', 'people', 'thinking']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "## Module constants\n",
    "lemmatizer  = WordNetLemmatizer()\n",
    "stopwords   = set(nltk.corpus.stopwords.words('english'))\n",
    "punctuation = string.punctuation\n",
    "\n",
    "def tagwn(tag):\n",
    "    \"\"\"\n",
    "    Returns the WordNet tag from the Penn Treebank tag.\n",
    "    \"\"\"\n",
    "\n",
    "    return {\n",
    "        'N': wn.NOUN,\n",
    "        'V': wn.VERB,\n",
    "        'R': wn.ADV,\n",
    "        'J': wn.ADJ\n",
    "    }.get(tag[0], wn.NOUN)\n",
    "\n",
    "\n",
    "def normalize(text):\n",
    "    for token, tag in nltk.pos_tag(nltk.wordpunct_tokenize(text)):\n",
    "        #if you're going to do part of speech tagging, do it here\n",
    "        token = token.lower()\n",
    "        if token in stopwords or token in punctuation:\n",
    "            continue\n",
    "        token = lemmatizer.lemmatize(token, tagwn(tag))\n",
    "        yield token\n",
    "\n",
    "print(list(normalize(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'for', \"should've\", 'hadn', 'can', 'themselves', 'doing', 'didn', 're', 'what', 'our', 'do', 'hasn', \"wouldn't\", 'over', 'under', 'again', 'above', 'where', 'having', 'ourselves', \"she's\", 've', 'during', 'isn', \"mightn't\", 'weren', 'why', 'before', 'each', 'out', 'himself', 'those', 'been', 'be', 'her', 't', 'ours', 'your', 'yours', 'only', \"that'll\", 'i', 'against', 'because', 'now', 'won', 'you', 'there', 'its', 'them', 'which', 'in', \"weren't\", 'own', 'did', \"you'd\", 'than', 'most', 'about', 'and', 'haven', 'whom', 'at', 'doesn', 'other', 'the', 'couldn', 'but', 'some', 'then', 'until', 'how', 'am', 'into', 'has', 'by', 'were', 'yourself', 'mightn', 'any', 'hers', 'this', 'ma', 'through', \"isn't\", 'from', 'myself', \"needn't\", 'between', \"you'll\", 'it', 'here', 'should', 'a', 'down', 'few', 'herself', 'that', 'are', 'all', 'too', 'nor', 'if', 'off', 'below', \"it's\", \"don't\", 'm', 'needn', 'does', 'or', 'while', 'wouldn', 'after', 'he', 'further', 'such', 'was', 'o', 'just', 'shan', \"aren't\", 'd', 'their', 'wasn', 'of', \"doesn't\", 'to', 'his', \"hadn't\", 'they', 'with', 'ain', 'shouldn', \"won't\", \"couldn't\", \"haven't\", 'itself', 'not', \"didn't\", 'don', \"mustn't\", 'yourselves', 'him', 'an', 's', 'aren', 'once', 'both', \"you're\", 'is', 'same', \"shan't\", \"you've\", 'y', 'more', 'she', 'as', 'me', 'up', \"hasn't\", \"wasn't\", 'my', 'theirs', 'when', 'these', 'being', 'who', 'so', 'very', 'no', \"shouldn't\", 'on', 'we', 'mustn', 'have', 'll', 'will', 'had'}\n"
     ]
    }
   ],
   "source": [
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named-entity recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Your/PRP$\n",
      "  time/NN\n",
      "  is/VBZ\n",
      "  limited/VBN\n",
      "  ,/,\n",
      "  so/RB\n",
      "  do/VBP\n",
      "  n't/RB\n",
      "  waste/VB\n",
      "  it/PRP\n",
      "  living/VBG\n",
      "  someone/NN\n",
      "  else/RB\n",
      "  's/POS\n",
      "  life/NN\n",
      "  ./.\n",
      "  Do/VBP\n",
      "  n't/RB\n",
      "  be/VB\n",
      "  trapped/VBN\n",
      "  by/IN\n",
      "  dogma/NN\n",
      "  –/NN\n",
      "  which/WDT\n",
      "  is/VBZ\n",
      "  living/VBG\n",
      "  with/IN\n",
      "  the/DT\n",
      "  results/NNS\n",
      "  of/IN\n",
      "  other/JJ\n",
      "  people/NNS\n",
      "  's/POS\n",
      "  thinking/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "text = sentences\n",
    "print(nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(text))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[O] Your\n",
      "[O] time\n",
      "[O] is\n",
      "[O] limited,\n",
      "[O] so\n",
      "[O] don't\n",
      "[O] waste\n",
      "[O] it\n",
      "[O] living\n",
      "[O] someone\n",
      "[O] else's\n",
      "[O] life.\n",
      "[O] Don't\n",
      "[O] be\n",
      "[O] trapped\n",
      "[O] by\n",
      "[O] dogma\n",
      "[O] –\n",
      "[O] which\n",
      "[O] is\n",
      "[O] living\n",
      "[O] with\n",
      "[O] the\n",
      "[O] results\n",
      "[O] of\n",
      "[O] other\n",
      "[O] people's\n",
      "[O] thinking.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "stanford_data = 'C:\\\\Users\\\\Woojin\\\\Desktop\\\\stanford-ner-4.0.0\\\\stanford-ner-4.0.0\\\\classifiers\\\\english.all.3class.distsim.crf.ser.gz'\n",
    "stanford_jar =  'C:\\\\Users\\\\Woojin\\\\Desktop\\\\stanford-ner-4.0.0\\\\stanford-ner-4.0.0\\\\stanford-ner-4.0.0.jar'\n",
    "\n",
    "text = sentences\n",
    "st = StanfordNERTagger(stanford_data, stanford_jar, 'utf-8')\n",
    "for i in st.tag(text.split()):\n",
    "    print('[' + i[1] + '] ' + i[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = nltk.grammar.CFG.fromstring(\"\"\"\n",
    "\n",
    "S -> NP PUNCT | NP\n",
    "NP -> N N | ADJP NP | DET N | DET ADJP\n",
    "ADJP -> ADJ NP | ADJ N\n",
    "\n",
    "DET -> 'an' | 'the' | 'a' | 'that' | 'my'\n",
    "N -> 'airplane' | 'runway' | 'face' | 'chair' | 'person' \n",
    "ADJ -> 'red' | 'slow' | 'tired' | 'handsome'\n",
    "PUNCT -> '.'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP (DET my) (ADJP (ADJ handsome) (N face))))\n"
     ]
    }
   ],
   "source": [
    "def parse(sent):\n",
    "    sent = sent.lower()\n",
    "    parser = nltk.parse.ChartParser(grammar)\n",
    "    for p in parser.parse(nltk.word_tokenize(sent)):\n",
    "        yield p \n",
    "\n",
    "        \n",
    "for tree in parse(\"my handsome face\"): \n",
    "    tree.pprint()\n",
    "#     tree[0].draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Woojin\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: The StanfordParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPParser\u001b[0m instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT\n",
      "  (S\n",
      "    (NP (PRP$ My) (JJ gorgeous) (NN face))\n",
      "    (VP\n",
      "      (VBZ is)\n",
      "      (ADJP (ADJP (JJR brighter)) (PP (IN in) (NP (NN night)))))\n",
      "    (. .)))\n"
     ]
    }
   ],
   "source": [
    "from nltk.parse.stanford import StanfordParser\n",
    "\n",
    "stanford_model = 'C:\\\\Users\\\\Woojin\\\\Desktop\\\\stanford-parser-4.0.0\\\\stanford-parser-4.0.0\\\\stanford-parser-4.0.0-models.jar'\n",
    "stanford_jar = 'C:\\\\Users\\\\Woojin\\\\Desktop\\\\stanford-parser-4.0.0\\\\stanford-parser-4.0.0\\\\stanford-parser.jar'\n",
    "\n",
    "st = StanfordParser(stanford_model, stanford_jar)\n",
    "sent = \"My gorgeous face is brighter in night.\"\n",
    "for tree in st.parse(nltk.wordpunct_tokenize(sent)):\n",
    "    tree.pprint()\n",
    "#     tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
